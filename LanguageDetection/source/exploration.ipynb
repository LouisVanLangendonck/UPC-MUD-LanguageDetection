{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "data_path = os.path.join(os.getcwd(), '../data')\n",
    "df = pd.read_csv(os.path.join(data_path, 'dataset.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sebes joseph pereira thomas  på eng the jesuit...</td>\n",
       "      <td>Swedish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ถนนเจริญกรุง อักษรโรมัน thanon charoen krung เ...</td>\n",
       "      <td>Thai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>விசாகப்பட்டினம் தமிழ்ச்சங்கத்தை இந்துப் பத்திர...</td>\n",
       "      <td>Tamil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>de spons behoort tot het geslacht haliclona en...</td>\n",
       "      <td>Dutch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>エノが行きがかりでバスに乗ってしまい、気分が悪くなった際に助けるが、今すぐバスを降りたいと運...</td>\n",
       "      <td>Japanese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>tsutinalar i̇ngilizce tsuutina kanadada albert...</td>\n",
       "      <td>Turkish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>müller mox figura centralis circulorum doctoru...</td>\n",
       "      <td>Latin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>برقی بار electric charge تمام زیرجوہری ذرات کی...</td>\n",
       "      <td>Urdu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>シャーリー・フィールドは、サン・ベルナルド・アベニュー沿い市民センターとrtマーティン高校に...</td>\n",
       "      <td>Japanese</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  language\n",
       "1  sebes joseph pereira thomas  på eng the jesuit...   Swedish\n",
       "2  ถนนเจริญกรุง อักษรโรมัน thanon charoen krung เ...      Thai\n",
       "3  விசாகப்பட்டினம் தமிழ்ச்சங்கத்தை இந்துப் பத்திர...     Tamil\n",
       "4  de spons behoort tot het geslacht haliclona en...     Dutch\n",
       "5  エノが行きがかりでバスに乗ってしまい、気分が悪くなった際に助けるが、今すぐバスを降りたいと運...  Japanese\n",
       "6  tsutinalar i̇ngilizce tsuutina kanadada albert...   Turkish\n",
       "7  müller mox figura centralis circulorum doctoru...     Latin\n",
       "8  برقی بار electric charge تمام زیرجوہری ذرات کی...      Urdu\n",
       "9  シャーリー・フィールドは、サン・ベルナルド・アベニュー沿い市民センターとrtマーティン高校に...  Japanese"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aux = df[1:10]\n",
    "aux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'My name is X335. Nice to meet you! Today you turn 23 years old?'\n",
    "text2 = 'エノが行きがかりでバスに乗ってしまい、気分が悪くなった際に助けるが、今すぐバスを降りたいと運'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['My', 'name', 'is', 'X335', '.', 'Nice', 'to', 'meet', 'you', '!', 'Today', 'you', 'turn', '23', 'years', 'old', '?']\n",
      "['エノが行きがかりでバスに乗ってしまい、気分が悪くなった際に助けるが、今すぐバスを降りたいと運']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "print(nltk.tokenize.word_tokenize(text))\n",
    "print(nltk.tokenize.word_tokenize(text2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['my', 'name', 'is', 'nice', 'to', 'meet', 'you', 'today', 'you', 'turn', 'years', 'old']\n",
      "['エノが行きがかりでバスに乗ってしまい', '気分が悪くなった際に助けるが', '今すぐバスを降りたいと運']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = nltk.tokenize.RegexpTokenizer(r'\\b[^\\d\\W]+\\b')\n",
    "print(tokenizer.tokenize(text.lower()))\n",
    "print(tokenizer.tokenize(text2.lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>My name is X335. Nice to meet you! Today you t...</td>\n",
       "      <td>english</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text language\n",
       "0  My name is X335. Nice to meet you! Today you t...  english"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = {'Text': ['My name is X335. Nice to meet you! Today you turn 23 years old?'], 'language': ['english']}\n",
    "df = pd.DataFrame(data)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text        My name is X335. Nice to meet you! Today you t...\n",
       "language                                              english\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [My name is X335., Nice to meet you!, Today yo...\n",
       "Name: Text, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"Text\"].apply(lambda x : nltk.tokenize.sent_tokenize(x))#.apply(pd.Series,1).stack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Text'] = df['Text'].apply(lambda x : nltk.tokenize.sent_tokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.explode('Text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                My name is X335.\n",
       "0               Nice to meet you!\n",
       "0    Today you turn 23 years old?\n",
       "Name: Text, dtype: object"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[:, 'Text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'test tes test'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize_sentence(sentence):\n",
    "    import nltk\n",
    "    \n",
    "    tokenizer = nltk.tokenize.RegexpTokenizer(r'\\b[^\\d\\W]+\\b') # Splits text into the multiple non-numerical words contained in it\n",
    "    sentence = tokenizer.tokenize(sentence)\n",
    "\n",
    "    #porter_stem = nltk.stem.PorterStemmer() # Stems all the words using the Porter Stemmer algorithm\n",
    "    sentence = ' '.join(sentence) # The generated list is joined again as a string as CountVectorizer cannot properly read it otherwise\n",
    "\n",
    "    return sentence\n",
    "\n",
    "def preprocess(sentence, labels):\n",
    "    sentence = sentence.apply(tokenize_sentence) # Apply the auxiliary function to actually tokenize the sentences\n",
    "    return sentence,labels\n",
    "\n",
    "tokenize_sentence('test tes test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import unicodedata as ud\n",
    "\n",
    "latin_letters= {}\n",
    "\n",
    "def is_latin(uchr):\n",
    "    try: return latin_letters[uchr]\n",
    "    except KeyError:\n",
    "         return latin_letters.setdefault(uchr, 'LATIN' in ud.name(uchr))\n",
    "\n",
    "def is_greek(uchr):\n",
    "    try: return latin_letters[uchr]\n",
    "    except KeyError:\n",
    "         return latin_letters.setdefault(uchr, 'GREEK' in ud.name(uchr))\n",
    "\n",
    "def only_roman_chars(unistr):\n",
    "    return all(is_latin(uchr)\n",
    "           for uchr in unistr\n",
    "           if uchr.isalpha())\n",
    "    \n",
    "only_roman_chars(u\"ε\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import string\n",
    "\n",
    "def tokenize_sentence(sentence):\n",
    "    '''\n",
    "    Input:\n",
    "    Output:\n",
    "    '''\n",
    "    import nltk\n",
    "    \n",
    "    if contains_cjk(sentence):\n",
    "        alph_is_cjk, idxs = alphabet_is_cjk(sentence)\n",
    "        if alph_is_cjk:\n",
    "            sentence = list(sentence)\n",
    "            return sentence\n",
    "        else:\n",
    "            pass\n",
    "            \n",
    "    tokenizer = nltk.tokenize.RegexpTokenizer(r'\\b[^\\d\\W]+\\b') # Splits text into the multiple non-numerical words contained in it\n",
    "    sentence = tokenizer.tokenize(sentence)\n",
    "\n",
    "    porter_stem = nltk.stem.PorterStemmer() # Stems all the words using the Porter Stemmer algorithm\n",
    "    sentence = ' '.join(porter_stem.stem(word) for word in sentence) # The generated list is joined again as a string as CountVectorizer cannot properly read it otherwise\n",
    "\n",
    "    return sentence\n",
    "\n",
    "def contains_cjk(unistr):\n",
    "    '''\n",
    "    Checks if a sentence contains chinese, japanese, or korean characters.\n",
    "    '''\n",
    "    import unicodedata as ud\n",
    "    \n",
    "    for uchr in unistr:\n",
    "        if any(x in ud.name(uchr) for x in ['CJK', 'HANGUL']):\n",
    "            return True\n",
    "        else:\n",
    "            continue\n",
    "    return False\n",
    "\n",
    "def contains_latin(unistr):\n",
    "    '''\n",
    "    Checks if a sentence contains latin characters.\n",
    "    '''\n",
    "    import unicodedata as ud\n",
    "    \n",
    "    for uchr in unistr:\n",
    "        if any(x in ud.name(uchr) for x in ['LATIN']):\n",
    "            return True\n",
    "        else:\n",
    "            continue\n",
    "    return False\n",
    "\n",
    "def alphabet_is_cjk(unistr):\n",
    "    '''\n",
    "    Checks if a sentence is mostly written in chinese, japanese, or korean characters.\n",
    "    '''\n",
    "    import unicodedata as ud\n",
    "    \n",
    "    cjk_chr = 0\n",
    "    other_chr = 0\n",
    "    idx_list = []\n",
    "    for idx, uchr in enumerate(unistr):\n",
    "        if any(x in ud.name(uchr) for x in ['CJK', 'HANGUL']):\n",
    "            cjk_chr += 1\n",
    "            idx_list.append(idx)\n",
    "        else:\n",
    "            other_chr += 1\n",
    "            \n",
    "    cjk_prop = cjk_chr/(other_chr+cjk_chr)\n",
    "    if cjk_prop >= 0.5:\n",
    "        return True, idx_list\n",
    "    \n",
    "    return False, idx_list\n",
    "\n",
    "def alphabet_is_latin(unistr):\n",
    "    '''\n",
    "    Checks if a sentence is mostly written in latin characters.\n",
    "    '''\n",
    "    import unicodedata as ud\n",
    "    \n",
    "    latin_chr = 0\n",
    "    other_chr = 0\n",
    "    idx_list = []\n",
    "    for idx, uchr in enumerate(unistr):\n",
    "        if any(x in ud.name(uchr) for x in ['LATIN']):\n",
    "            latin_chr += 1\n",
    "            idx_list.append(idx)\n",
    "        else:\n",
    "            other_chr += 1\n",
    "            \n",
    "    latin_prop = latin_chr/(other_chr+latin_chr)\n",
    "    if latin_prop >= 0.5:\n",
    "        return True, idx_list\n",
    "    \n",
    "    return False, idx_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_sentence(sentence):\n",
    "    '''\n",
    "    Input:\n",
    "    Output:\n",
    "    '''\n",
    "    import nltk\n",
    "    \n",
    "    if contains_cjk(sentence):\n",
    "        alph_is_cjk, idxs = alphabet_is_cjk(sentence)\n",
    "        if alph_is_cjk:\n",
    "            final_sentence = ''\n",
    "            for idx in idxs:\n",
    "                final_sentence += sentence[idx] + ' '\n",
    "            return final_sentence\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    elif contains_latin(sentence):\n",
    "        alph_is_latin, idxs = alphabet_is_latin(sentence)\n",
    "        if alph_is_latin:\n",
    "            final_sentence = ''\n",
    "            for idx in idxs:\n",
    "                final_sentence += sentence[idx]\n",
    "            sentence = final_sentence\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    tokenizer = nltk.tokenize.RegexpTokenizer(r'\\b[^\\d\\W]+\\b') # Splits text into the multiple non-numerical words contained in it\n",
    "    sentence = tokenizer.tokenize(sentence)\n",
    "\n",
    "    porter_stem = nltk.stem.PorterStemmer() # Stems all the words using the Porter Stemmer algorithm\n",
    "    sentence = ' '.join(porter_stem.stem(word) for word in sentence) # The generated list is joined again as a string as CountVectorizer cannot properly read it otherwise\n",
    "\n",
    "    return sentence\n",
    "\n",
    "def contains_cjk(unistr):\n",
    "    '''\n",
    "    Checks if a sentence contains chinese, japanese, or korean characters.\n",
    "    '''\n",
    "    import unicodedata as ud\n",
    "    \n",
    "    for uchr in unistr:\n",
    "        if any(x in ud.name(uchr) for x in ['CJK', 'HANGUL']):\n",
    "            return True\n",
    "        else:\n",
    "            continue\n",
    "    return False\n",
    "\n",
    "def contains_latin(unistr):\n",
    "    '''\n",
    "    Checks if a sentence contains latin characters.\n",
    "    '''\n",
    "    import unicodedata as ud\n",
    "    \n",
    "    for uchr in unistr:\n",
    "        if any(x in ud.name(uchr) for x in ['LATIN']):\n",
    "            return True\n",
    "        else:\n",
    "            continue\n",
    "    return False\n",
    "\n",
    "def alphabet_is_cjk(unistr):\n",
    "    '''\n",
    "    Checks if a sentence is mostly written in chinese, japanese, or korean characters.\n",
    "    '''\n",
    "    import unicodedata as ud\n",
    "    \n",
    "    cjk_chr = 0\n",
    "    other_chr = 0\n",
    "    idx_list = []\n",
    "    for idx, uchr in enumerate(unistr):\n",
    "        if any(x in ud.name(uchr) for x in ['CJK', 'HANGUL']):\n",
    "            cjk_chr += 1\n",
    "            idx_list.append(idx)\n",
    "        else:\n",
    "            other_chr += 1\n",
    "            \n",
    "    cjk_prop = cjk_chr/(other_chr+cjk_chr)\n",
    "    if cjk_prop >= 0.5:\n",
    "        return True, idx_list\n",
    "    \n",
    "    return False, idx_list\n",
    "\n",
    "def alphabet_is_latin(unistr):\n",
    "    '''\n",
    "    Checks if a sentence is mostly written in latin characters.\n",
    "    '''\n",
    "    import unicodedata as ud\n",
    "    \n",
    "    latin_chr = 0\n",
    "    other_chr = 0\n",
    "    idx_list = []\n",
    "    for idx, uchr in enumerate(unistr):\n",
    "        if any(x in ud.name(uchr) for x in ['LATIN']):\n",
    "            latin_chr += 1\n",
    "            idx_list.append(idx)\n",
    "        elif uchr == ' ':\n",
    "            idx_list.append(idx)\n",
    "        else:\n",
    "            other_chr += 1\n",
    "            \n",
    "    latin_prop = latin_chr/(other_chr+latin_chr)\n",
    "    if latin_prop >= 0.5:\n",
    "        return True, idx_list\n",
    "    \n",
    "    return False, idx_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a aسعدت بلسعدت بلقائكسعدت بلقائكaaaaaaa'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize_sentence('a  aسعدت بلسعدت بلقائكسعدت بلقائكaaaaaaa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CJK UNIFIED IDEOGRAPH-5F48'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ud.name('彈')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CJK UNIFIED IDEOGRAPH-8D5B'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ud.name('赛')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'THAI CHARACTER SO SUA'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ud.name('ส')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = {}\n",
    "l.setdefault('ې', 'ARABIC' in ud.name('ې'))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e488ff33f1448af5edd452d3cbe66aea966299ce1c28a510266e3273d74231aa"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 ('mud_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
